{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.rename(\"/content/file.env\", \"/content/.env\")"
      ],
      "metadata": {
        "id": "FVgkVcp9-Gap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUP1y3UvFi6J"
      },
      "outputs": [],
      "source": [
        "# Installing required modules\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community langchain"
      ],
      "metadata": {
        "id": "_f_1uUFQFsw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "G9tlbbnCF5Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "RztVG3KNG5tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will help if the key is saved in the .env file\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "Qw4TMWCzHOcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explicitly add key if not mentioned in the .env file\n",
        "ashu_key = \"<OPENAI_KEY>\""
      ],
      "metadata": {
        "id": "XisUED05HOZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=ashu_key) # to use key explicitly\n",
        "# client = OpenAI() # to use key from .env file"
      ],
      "metadata": {
        "id": "VbRi7CidHOW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-nano\",\n",
        "    input=\"What is a RAG in the context of LLM?\",\n",
        "    # max_output_tokens=1000\n",
        ")\n",
        "\n",
        "print(response.output_text)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0DdawA9VHReh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0CLALd4yV1Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templating using Langchain"
      ],
      "metadata": {
        "id": "mWcZ54rRZCkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "NRJWo90zHUy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating prompt from template\n",
        "my_query = \"What are LLM models available by {topic}. Describe one liner for each.\"\n",
        "# using prompt_template\n",
        "mytemplate = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=my_query\n",
        ")\n",
        "# prompt_template = PromptTemplate.from_template(my_query)"
      ],
      "metadata": {
        "id": "K-kMuR0rKOZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mytemplate"
      ],
      "metadata": {
        "id": "ut0AgewI9akZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling mytemplate to fill variable\n",
        "my_prompt = mytemplate.format(topic=\"OpenAI\")\n",
        "print(my_prompt)"
      ],
      "metadata": {
        "id": "GPHbSH2HKsHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_str = \"What are LLM models available by Anthropic. Describe one liner for each.\"\n",
        "# Langchain supports agent, RAG, execution, flow chain\n",
        "# There is no native support in str for LLM\n",
        "# Template can be stored in hub\n",
        "# Variables is checked at compile time itself\n",
        "# created promts can be shared in community"
      ],
      "metadata": {
        "id": "9QWJBzDMLODd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5-nano\",\n",
        "    input=my_prompt,\n",
        "    # input=my_str,\n",
        "    # max_output_tokens=1000\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "39F9iyiqK7cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to invoke LLM"
      ],
      "metadata": {
        "id": "oIBzD4wOX2Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use langchain to call LLM itself\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0.5,\n",
        "    model=\"gpt-4.1\",\n",
        "    api_key=ashu_key\n",
        ")"
      ],
      "metadata": {
        "id": "QzXtCQZGLD7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(my_prompt)"
      ],
      "metadata": {
        "id": "ib5nMmTVNxcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using llm to call template\n",
        "response1 = llm.invoke(my_prompt)"
      ],
      "metadata": {
        "id": "3hNwgWuCMolT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response1.content)"
      ],
      "metadata": {
        "id": "CdUlTJIxM6QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnr9iMDPNl_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pT5hdwUYfChH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector embedding for semantic **similarity**"
      ],
      "metadata": {
        "id": "_qvp4UAafLch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector embedding for semantic similarity"
      ],
      "metadata": {
        "id": "tuz0LrJvfOiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing scikit learn module"
      ],
      "metadata": {
        "id": "EWArJDD0fX_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "__ZcR48efbNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=ashu_key)\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input = [text], model=model)\n",
        "\n",
        "    return np.array(response.data[0].embedding)\n",
        "\n",
        "# df['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
        "# df.to_csv('output/embedded_1k_reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "Tofi3HjGfgnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Exploring and learning RAG is fun!\"\n",
        "text2 = \"FAISS is one of the embeddings that can be used in building RAG\"\n",
        "text3 = \"I am big fan of Sachin Tendulkar!\""
      ],
      "metadata": {
        "id": "mlO2Hdc6gZWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the embedding of texts"
      ],
      "metadata": {
        "id": "s01tz_GGgkSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb1 = get_embedding(text1)\n",
        "emb2 = get_embedding(text2)\n",
        "emb3 = get_embedding(text3)"
      ],
      "metadata": {
        "id": "vLUaBuw5gm9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(emb1)"
      ],
      "metadata": {
        "id": "DKXEHRg0grpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling similarity\n",
        "similarity1 = cosine_similarity([emb1], [emb2])\n",
        "similarity2 = cosine_similarity([emb1], [emb3])\n",
        "similarity3 = cosine_similarity([emb3], [emb2])\n"
      ],
      "metadata": {
        "id": "-U016XT5gsXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print similarity\n",
        "print(f\"Similarity between text1 and text2: {similarity1[0][0]}\")\n",
        "print(f\"Similarity between text1 and text3: {similarity2[0][0]}\")\n",
        "print(f\"Similarity between text3 and text2: {similarity3[0][0]}\")"
      ],
      "metadata": {
        "id": "OLRhBaXbg0-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSNTFOf9heyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing RAG with external CSV: Using FAISS"
      ],
      "metadata": {
        "id": "CMaiFBLAY4n8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pah8OLVImmES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing required library\n",
        "!pip install faiss-cpu pandas openai langchain-community langchain python-dotenv scikit-learn langchain_openai"
      ],
      "metadata": {
        "id": "dA6lMIXbmpSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required modules"
      ],
      "metadata": {
        "id": "2eEXWHtmm8RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "BCFp3q4KnNRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading csv file\n",
        "df = pd.read_csv(\"user.csv\")\n",
        "df.info()\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "42r3NZisnPgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting dataframe to langchain document\n",
        "loader = DataFrameLoader(\n",
        "    df, page_content_column=\"name\"\n",
        ")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "9jjlpkvynXuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader"
      ],
      "metadata": {
        "id": "NOKee09_oWU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# before embedding we need to tokenization\n"
      ],
      "metadata": {
        "id": "X4ouDQfooFfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split into chunks (though small, we'll treat each row as a chunk)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        ")\n",
        "chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "a66STx9Rpy0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    # openai_api_key=os.getenv(\"OPENAI_API_KEY\")  # or use the direct variable\n",
        "    openai_api_key=ashu_key\n",
        ")\n"
      ],
      "metadata": {
        "id": "lZZ9KRHGpzSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "hDTMDXeRp1Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings.json())"
      ],
      "metadata": {
        "id": "mGqVU9ThqbZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "KXEP0Mh6qh2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aacc8fed"
      },
      "source": [
        "# store data in any vector DB\n",
        "# Create vector DB to store vector in FAISS\n",
        "vector_db = FAISS.from_documents(chunks, embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ac087b4"
      },
      "source": [
        "print(vector_db)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the number of vectors in the FAISS index:\n",
        "print(vector_db.index.ntotal)"
      ],
      "metadata": {
        "id": "M7hw5Err_05o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define rag prompt template\n",
        "# Step 7: Define RAG prompt\n",
        "template = \"\"\"Answer the question based only on the following employee database context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Format your answer with these details:\n",
        "- Name: [full name]\n",
        "- Email: [email]\n",
        "- Department: [department]\n",
        "- Position: [position]\n",
        "- Salary: [salary]\n",
        "- Hire Date: [hire_date]\n",
        "\n",
        "If multiple employees match, list them all.\n",
        "\n",
        "If the user is asking very specific question or field then no need to follow the format above.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "tn2MZEFi_33M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "id": "WuNRqccFAkDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling any ChatOpenAI\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', openai_api_key=ashu_key)"
      ],
      "metadata": {
        "id": "EI9S38d9AmPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create RAG chain to call prompt template + LLM\n",
        "# chain = prompt | llm # this is okay to call it\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": vector_db.as_retriever(search_kwargs={\"k\": 30}),  # Retrieve top 3 matches\n",
        "    \"question\": RunnablePassthrough()\n",
        "} | prompt | llm)"
      ],
      "metadata": {
        "id": "C7ZCuYpvAtiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9xfYgKqNAtSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"Can you print distinct positions of employee?\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "kZVsaeV7OqiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"Compare all salaries and find Who has the highest salary. share details of only that employee\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "KfJPuo1UBI4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"Explain machine learning.\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "KzuokhoRBODq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"Tell me something about employee.\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "DcBSWjXHBR88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}